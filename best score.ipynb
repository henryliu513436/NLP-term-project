{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"},{"sourceId":9102725,"sourceType":"datasetVersion","datasetId":5493674},{"sourceId":9107824,"sourceType":"datasetVersion","datasetId":5496762},{"sourceId":9107963,"sourceType":"datasetVersion","datasetId":5496847},{"sourceId":9108069,"sourceType":"datasetVersion","datasetId":5496920},{"sourceId":10223970,"sourceType":"datasetVersion","datasetId":6320575},{"sourceId":10296256,"sourceType":"datasetVersion","datasetId":6372725}],"dockerImageVersionId":30747,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Credits goes to @tascj0 [is just copy of his team best solution](https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/527685)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-05T07:35:10.240978Z","iopub.execute_input":"2024-08-05T07:35:10.241976Z","iopub.status.idle":"2024-08-05T07:35:10.247863Z","shell.execute_reply.started":"2024-08-05T07:35:10.241927Z","shell.execute_reply":"2024-08-05T07:35:10.245857Z"}}},{"cell_type":"markdown","source":"# Packages","metadata":{}},{"cell_type":"code","source":"%pip install /kaggle/input/lmsys-packages/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%pip install /kaggle/input/lmsys-packages/xformers-0.0.24042abc8.d20240802-cp310-cp310-linux_x86_64.whl","metadata":{"execution":{"iopub.status.busy":"2024-10-24T12:16:35.799429Z","iopub.execute_input":"2024-10-24T12:16:35.799746Z","iopub.status.idle":"2024-10-24T12:17:15.141706Z","shell.execute_reply.started":"2024-10-24T12:16:35.799718Z","shell.execute_reply":"2024-10-24T12:17:15.140611Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cp -r /kaggle/input/lmsys-modules-0805 human_pref","metadata":{"execution":{"iopub.status.busy":"2024-10-24T12:17:15.142996Z","iopub.execute_input":"2024-10-24T12:17:15.143299Z","iopub.status.idle":"2024-10-24T12:17:16.190536Z","shell.execute_reply.started":"2024-10-24T12:17:15.14327Z","shell.execute_reply":"2024-10-24T12:17:16.189428Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prepare test file","metadata":{}},{"cell_type":"code","source":"%%writefile prepare_test_file.py\nimport pandas as pd\n\n\ndf = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\ndf[\"winner_model_a\"] = 1\ndf[\"winner_model_b\"] = 0\ndf[\"winner_tie\"] = 0\ndf.to_parquet(\"test.parquet\", index=False)\n\ndf[\"response_a\"], df[\"response_b\"] = df[\"response_b\"], df[\"response_a\"]\ndf.to_parquet(\"test_swap.parquet\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T12:17:16.193422Z","iopub.execute_input":"2024-10-24T12:17:16.193869Z","iopub.status.idle":"2024-10-24T12:17:16.201923Z","shell.execute_reply.started":"2024-10-24T12:17:16.193828Z","shell.execute_reply":"2024-10-24T12:17:16.200952Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python prepare_test_file.py","metadata":{"execution":{"iopub.status.busy":"2024-10-24T12:17:16.203101Z","iopub.execute_input":"2024-10-24T12:17:16.20342Z","iopub.status.idle":"2024-10-24T12:17:18.0008Z","shell.execute_reply.started":"2024-10-24T12:17:16.20339Z","shell.execute_reply":"2024-10-24T12:17:17.999649Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference: gemma2-9b","metadata":{}},{"cell_type":"code","source":"%%writefile predict_m0.py\nimport torch\nimport numpy as np\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer\n\n# from xformers.ops.fmha.attn_bias import BlockDiagonalCausalMask\nfrom human_pref.models.modeling_gemma2 import Gemma2ForSequenceClassification\nfrom human_pref.data.processors import ProcessorPAB\nfrom human_pref.data.dataset import LMSYSDataset\nfrom human_pref.data.collators import VarlenCollator, ShardedMaxTokensCollator\nfrom human_pref.utils import to_device\n\n\nmodel_name_or_path = \"/kaggle/input/lmsys-checkpoints-0-0805\"\ncsv_path = \"test.parquet\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\nprocessor = ProcessorPAB(\n    tokenizer=tokenizer,\n    max_length=4096,\n    support_system_role=False,\n)\ndataset = LMSYSDataset(\n    csv_file=csv_path,\n    query=None,\n    processor=processor,\n    include_swap=False,\n    is_parquet=True,\n)\ndataloader = DataLoader(\n    dataset,\n    batch_size=80,\n    num_workers=4,\n    collate_fn=ShardedMaxTokensCollator(\n        max_tokens=8192, base_collator=VarlenCollator()\n    ),\n)\n\n# model for pipelined inference\nnum_hidden_layers = 42\ndevice_map = {\n    \"model.embed_tokens\": \"cuda:0\",\n    \"model.norm\": \"cuda:1\",\n    \"score\": \"cuda:1\",\n}\nfor i in range(num_hidden_layers // 2):\n    device_map[f\"model.layers.{i}\"] = \"cuda:0\"\nfor i in range(num_hidden_layers // 2, num_hidden_layers):\n    device_map[f\"model.layers.{i}\"] = \"cuda:1\"\n\nmodel = Gemma2ForSequenceClassification.from_pretrained(\n    model_name_or_path,\n    torch_dtype=torch.float16,\n    device_map=device_map,\n)\n\n# inv_freq clones for each device\nconfig = model.config\ndim = config.head_dim\ninv_freq = 1.0 / (\n    config.rope_theta ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim)\n)\ninv_freq0 = inv_freq.to(\"cuda:0\")\ninv_freq1 = inv_freq.to(\"cuda:1\")\n\n\n# for name, p in model.named_parameters():\n#     print(name, p.device)\n# for name, b in model.model.named_buffers():\n#     print(name, b.device)\n\n# pipeline parallelism with two GPUs\nis_first = True\nhidden_states = None\nouts = []\nfor batch in tqdm(dataloader):\n    for micro_batch in batch:\n        input_ids = to_device(micro_batch[\"input_ids\"], \"cuda:0\")\n        seq_info = dict(\n            cu_seqlens=micro_batch[\"cu_seqlens\"],\n            position_ids=micro_batch[\"position_ids\"],\n            max_seq_len=micro_batch[\"max_seq_len\"],\n            # attn_bias=BlockDiagonalCausalMask.from_seqlens(micro_batch[\"seq_lens\"]),\n        )\n        seq_info = to_device(seq_info, \"cuda:0\")\n        if is_first:\n            with torch.no_grad(), torch.cuda.amp.autocast():\n                prev_hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n            is_first = False\n            prev_seq_info, prev_hidden_states = to_device(\n                [seq_info, prev_hidden_states], \"cuda:1\"\n            )\n            continue\n        with torch.no_grad(), torch.cuda.amp.autocast():\n            logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n            hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n\n            prev_seq_info, prev_hidden_states = to_device(\n                [seq_info, hidden_states], \"cuda:1\"\n            )\n            outs.append(logits.cpu())\n\n# last micro-batch\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n    outs.append(logits.cpu())\n\npred = torch.cat(outs, dim=0)\nprob = pred.softmax(-1)\nprint(dataset.evaluate(prob.numpy()))\n\nnp.save('prob_m0.npy', prob)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T12:17:18.002293Z","iopub.execute_input":"2024-10-24T12:17:18.002622Z","iopub.status.idle":"2024-10-24T12:17:18.011416Z","shell.execute_reply.started":"2024-10-24T12:17:18.002594Z","shell.execute_reply":"2024-10-24T12:17:18.010376Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python predict_m0.py","metadata":{"execution":{"iopub.status.busy":"2024-10-24T12:17:18.012512Z","iopub.execute_input":"2024-10-24T12:17:18.012799Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference: llama3-8b","metadata":{}},{"cell_type":"code","source":"%%writefile predict_m3.py\nimport torch\nimport numpy as np\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer\n\nfrom xformers.ops.fmha.attn_bias import BlockDiagonalCausalMask\nfrom human_pref.models.modeling_llama import LlamaForSequenceClassification\nfrom human_pref.data.processors import ProcessorPAB\nfrom human_pref.data.dataset import LMSYSDataset\nfrom human_pref.data.collators import VarlenCollator, ShardedMaxTokensCollator\nfrom human_pref.utils import to_device\n\n\nmodel_name_or_path = \"/kaggle/input/lmsys-checkpoints-3-0805\"\ncsv_path = \"test_swap.parquet\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\ntokenizer.deprecation_warnings[\n    \"sequence-length-is-longer-than-the-specified-maximum\"\n] = True\nprocessor = ProcessorPAB(\n    tokenizer=tokenizer,\n    max_length=4096,\n    support_system_role=True,\n)\ndataset = LMSYSDataset(\n    csv_file=csv_path,\n    query=None,\n    processor=processor,\n    include_swap=False,\n    is_parquet=True,\n)\ndataloader = DataLoader(\n    dataset,\n    batch_size=80,\n    num_workers=4,\n    collate_fn=ShardedMaxTokensCollator(\n        max_tokens=8192, base_collator=VarlenCollator()\n    ),\n)\n\n# model for pipelined inference\nnum_hidden_layers = 32\ndevice_map = {\n    \"model.embed_tokens\": \"cuda:0\",\n    \"model.norm\": \"cuda:1\",\n    \"score\": \"cuda:1\",\n}\nfor i in range(num_hidden_layers // 2):\n    device_map[f\"model.layers.{i}\"] = \"cuda:0\"\nfor i in range(num_hidden_layers // 2, num_hidden_layers):\n    device_map[f\"model.layers.{i}\"] = \"cuda:1\"\n\nmodel = LlamaForSequenceClassification.from_pretrained(\n    model_name_or_path,\n    torch_dtype=torch.float16,\n    device_map=device_map,\n)\n\n# inv_freq clones for each device\nconfig = model.config\ndim = config.hidden_size // config.num_attention_heads\ninv_freq = 1.0 / (\n    config.rope_theta ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim)\n)\ninv_freq0 = inv_freq.to(\"cuda:0\")\ninv_freq1 = inv_freq.to(\"cuda:1\")\n\n\n# for name, p in model.named_parameters():\n#     print(name, p.device)\n# for name, b in model.model.named_buffers():\n#     print(name, b.device)\n\n# pipeline parallelism with two GPUs\nis_first = True\nhidden_states = None\nouts = []\nfor batch in tqdm(dataloader):\n    for micro_batch in batch:\n        input_ids = to_device(micro_batch[\"input_ids\"], \"cuda:0\")\n        seq_info = dict(\n            cu_seqlens=micro_batch[\"cu_seqlens\"],\n            position_ids=micro_batch[\"position_ids\"],\n            max_seq_len=micro_batch[\"max_seq_len\"],\n            attn_bias=BlockDiagonalCausalMask.from_seqlens(micro_batch[\"seq_lens\"]),\n        )\n        seq_info = to_device(seq_info, \"cuda:0\")\n        if is_first:\n            with torch.no_grad(), torch.cuda.amp.autocast():\n                prev_hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n            is_first = False\n            prev_seq_info, prev_hidden_states = to_device(\n                [seq_info, prev_hidden_states], \"cuda:1\"\n            )\n            continue\n        with torch.no_grad(), torch.cuda.amp.autocast():\n            logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n            hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n\n            prev_seq_info, prev_hidden_states = to_device(\n                [seq_info, hidden_states], \"cuda:1\"\n            )\n            outs.append(logits.cpu())\n\n# last micro-batch\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n    outs.append(logits.cpu())\n\n\npred = torch.cat(outs, dim=0)\nprob = pred.softmax(-1)\nprint(dataset.evaluate(prob.numpy()))\n\nnp.save('prob_m3.npy', prob)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python predict_m3.py","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T06:14:44.619181Z","iopub.execute_input":"2024-12-08T06:14:44.619497Z","iopub.status.idle":"2024-12-08T06:14:45.703487Z","shell.execute_reply.started":"2024-12-08T06:14:44.619472Z","shell.execute_reply":"2024-12-08T06:14:45.702851Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import set_seed\nimport ctypes, gc\nimport torch\nimport random\nimport numpy as np\n\nlibc = ctypes.CDLL(\"libc.so.6\")\n# Seed the same seed to all \ndef seed_everything(seed=42):\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    set_seed(seed)\n    \ndef clear_memory():\n    libc.malloc_trim(0)\n    torch.cuda.empty_cache()\n    gc.collect()\n\nSEED = 42\nseed_everything(SEED)\n# Set the GPUs\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.metrics import log_loss\nimport joblib","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T06:15:12.972259Z","iopub.execute_input":"2024-12-08T06:15:12.973277Z","iopub.status.idle":"2024-12-08T06:15:14.363663Z","shell.execute_reply.started":"2024-12-08T06:15:12.973243Z","shell.execute_reply":"2024-12-08T06:15:14.362781Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = pd.read_csv(r'/kaggle/input/llm-classification-finetuning/train.csv')\ntest = pd.read_csv(r'/kaggle/input/llm-classification-finetuning/test.csv')\nsample_submission = pd.read_csv(r'/kaggle/input/llm-classification-finetuning/sample_submission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T06:15:17.091450Z","iopub.execute_input":"2024-12-08T06:15:17.092372Z","iopub.status.idle":"2024-12-08T06:15:20.159543Z","shell.execute_reply.started":"2024-12-08T06:15:17.092346Z","shell.execute_reply":"2024-12-08T06:15:20.158610Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom scipy.sparse import hstack\nfrom lightgbm import LGBMClassifier\nimport gc\n\ny = train[['winner_model_a', 'winner_model_b', 'winner_tie']].idxmax(axis=1)\ny = y.map({\n    'winner_model_a': 0,\n    'winner_model_b': 1,\n    'winner_tie': 2\n})\n\ndf_fit = test if len(test) > 3 else train\n\ntfidf_prompt = TfidfVectorizer(\n    max_features=500,\n    stop_words='english',\n    min_df=0.002,\n    ngram_range=(1, 3)\n)\ncount_prompt = CountVectorizer(\n    max_features=500,\n    stop_words='english',\n    min_df=0.002,\n    ngram_range=(1, 3)\n)\n\ntfidf_response = TfidfVectorizer(\n    max_features=1000,\n    stop_words='english',\n    min_df=0.002,\n    ngram_range=(1, 3)\n)\ncount_response = CountVectorizer(\n    max_features=1000,\n    stop_words='english',\n    min_df=0.002,\n    ngram_range=(1, 3)\n)\n\ntfidf_prompt_char = TfidfVectorizer(\n    analyzer='char',\n    ngram_range=(1, 3),\n    max_features=500,\n    min_df=0.002\n)\ncount_prompt_char = CountVectorizer(\n    analyzer='char',\n    ngram_range=(1, 3),\n    max_features=500,\n    min_df=0.002\n)\n\ntfidf_response_char = TfidfVectorizer(\n    analyzer='char',\n    ngram_range=(1, 3),\n    max_features=1000,\n    min_df=0.002\n)\ncount_response_char = CountVectorizer(\n    analyzer='char',\n    ngram_range=(1, 3),\n    max_features=1000,\n    min_df=0.002\n)\n\ntfidf_prompt.fit(df_fit['prompt'])\ncount_prompt.fit(df_fit['prompt'])\n\ntfidf_response.fit(pd.concat([df_fit['response_a'], df_fit['response_b']]))\ncount_response.fit(pd.concat([df_fit['response_a'], df_fit['response_b']]))\n\ntfidf_prompt_char.fit(df_fit['prompt'])\ncount_prompt_char.fit(df_fit['prompt'])\n\ntfidf_response_char.fit(pd.concat([df_fit['response_a'], df_fit['response_b']]))\ncount_response_char.fit(pd.concat([df_fit['response_a'], df_fit['response_b']]))\n\ndef get_features(df):\n    X_prompt_tfidf = tfidf_prompt.transform(df['prompt'])\n    X_prompt_count = count_prompt.transform(df['prompt'])\n    \n    X_prompt_tfidf_char = tfidf_prompt_char.transform(df['prompt'])\n    X_prompt_count_char = count_prompt_char.transform(df['prompt'])\n    \n    X_prompt_combined = hstack([\n        X_prompt_tfidf, \n        X_prompt_count, \n        X_prompt_tfidf_char, \n        X_prompt_count_char\n    ])\n    \n    response_combined = pd.concat([df['response_a'], df['response_b']], axis=0)\n    X_response_tfidf = tfidf_response.transform(response_combined)\n    X_response_count = count_response.transform(response_combined)\n    \n    X_response_tfidf_char = tfidf_response_char.transform(response_combined)\n    X_response_count_char = count_response_char.transform(response_combined)\n    \n    n = len(df)\n    X_response_a_tfidf = X_response_tfidf[:n]\n    X_response_b_tfidf = X_response_tfidf[n:]\n    \n    X_response_a_count = X_response_count[:n]\n    X_response_b_count = X_response_count[n:]\n    \n    X_response_a_tfidf_char = X_response_tfidf_char[:n]\n    X_response_b_tfidf_char = X_response_tfidf_char[n:]\n    \n    X_response_a_count_char = X_response_count_char[:n]\n    X_response_b_count_char = X_response_count_char[n:]\n    \n    afeat = hstack([\n        X_response_a_tfidf, \n        X_response_a_count,\n        X_response_a_tfidf_char,\n        X_response_a_count_char\n    ])\n    bfeat = hstack([\n        X_response_b_tfidf, \n        X_response_b_count,\n        X_response_b_tfidf_char,\n        X_response_b_count_char\n    ])\n    \n    v = hstack([\n        afeat,\n        bfeat\n    ])\n    \n    extras = []\n    EXTRAS = ['\\n', '\\n\\n', '.', ' ', '\",\"']\n    for e in EXTRAS:\n        for c in ['prompt', 'response_a', 'response_b']:\n            extras.append(df[c].str.count(e).values)\n    \n    extras.append(df['prompt'].str.len().values)\n    extras.append(df['prompt'].str.split().apply(lambda x: len(x)).values)\n    \n    extras = np.stack(extras, axis=1)\n    extras = np.hstack([extras ** 0.5, np.log1p(extras)])\n    \n    final_features = hstack([v, extras, X_prompt_combined])\n    return final_features.tocsr()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T06:15:21.137089Z","iopub.execute_input":"2024-12-08T06:15:21.137649Z","iopub.status.idle":"2024-12-08T06:24:50.178446Z","shell.execute_reply.started":"2024-12-08T06:15:21.137621Z","shell.execute_reply":"2024-12-08T06:24:50.177445Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train = get_features(train)\nX_test = get_features(test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T06:29:13.012768Z","iopub.execute_input":"2024-12-08T06:29:13.013876Z","iopub.status.idle":"2024-12-08T06:36:56.815612Z","shell.execute_reply.started":"2024-12-08T06:29:13.013847Z","shell.execute_reply":"2024-12-08T06:36:56.814605Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_models_and_weights(save_path='/kaggle/input/model-and-weights-more-param/models_and_weights_more_param.joblib'):\n    models_and_weights = joblib.load(save_path)\n    models = models_and_weights['models']\n    weights = models_and_weights['weights']\n    print(f'Loaded {len(models)} models with corresponding weights.')\n    return models, weights\n\ndef weighted_predict(models, weights, X_test):\n    test_preds = np.zeros((X_test.shape[0], 3))\n    for model, weight in zip(models, weights):\n        y_pred = model.predict_proba(X_test)\n        test_preds += y_pred * weight\n    test_preds_weighted = test_preds / np.sum(weights)\n    return test_preds_weighted\n\nmodels, weights = load_models_and_weights(save_path='/kaggle/input/model-and-weights-more-param/models_and_weights_more_param.joblib')\n\ny_test_proba = weighted_predict(models, weights, X_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Make submission","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\ndf = pd.read_parquet(\"test.parquet\")\npreds = np.average(\n    [\n        np.load(\"prob_m0.npy\"),\n        np.load(\"prob_m3.npy\")[:, [1, 0, 2]],\n        y_test_proba\n    ],\n    axis=0,\n    weights=[50, 42, 0.5],\n)\nsub = pd.DataFrame({\n    \"id\": df[\"id\"],\n    \"winner_model_a\": preds[:, 0],\n    \"winner_model_b\": preds[:, 1],\n    \"winner_tie\": preds[:, 2],\n})\nsub.to_csv(\"submission.csv\", index=False)\nprint(sub.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}